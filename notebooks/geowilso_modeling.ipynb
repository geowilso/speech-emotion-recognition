{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e832cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:33:41.521411: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-24 21:33:41.521446: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras import utils\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout,\n",
    "                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation, Input, Dense)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import scipy\n",
    "from scipy.stats import skew\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import warnings\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8a1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data Augmentation method   \n",
    "'''\n",
    "def speedNpitch(data):\n",
    "    \"\"\"\n",
    "    Speed and Pitch Tuning.\n",
    "    \"\"\"\n",
    "    # you can change low and high here\n",
    "    length_change = np.random.uniform(low=0.8, high = 1)\n",
    "    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n",
    "    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n",
    "    minlen = min(data.shape[0], tmp.shape[0])\n",
    "    data *= 0\n",
    "    data[0:minlen] = tmp[0:minlen]\n",
    "    return data\n",
    "\n",
    "'''\n",
    "2. Extracting the MFCC feature as an image (Matrix format).  \n",
    "'''\n",
    "def prepare_data(df, n, aug, mfcc):\n",
    "    X = np.empty(shape=(df.shape[0], n, 862, 1))\n",
    "    input_length = sampling_rate * audio_duration\n",
    "    \n",
    "    cnt = 0\n",
    "    for fname in tqdm(df.path):\n",
    "        file_path = fname\n",
    "        data, _ = librosa.load(file_path, sr=sampling_rate\n",
    "                               ,res_type=\"kaiser_fast\"\n",
    "                               ,duration=10\n",
    "                               ,offset=0.1\n",
    "                              )\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, int(input_length) - len(data) - offset), \"constant\")\n",
    "\n",
    "        # Augmentation? \n",
    "        if aug == 1:\n",
    "            data = speedNpitch(data)\n",
    "        \n",
    "        # which feature?\n",
    "        if mfcc == 1:\n",
    "            # MFCC extraction \n",
    "            MFCC = librosa.feature.mfcc(data, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "            MFCC = np.expand_dims(MFCC, axis=-1)\n",
    "            X[cnt,] = MFCC\n",
    "            \n",
    "        else:\n",
    "            # Log-melspectogram\n",
    "            melspec = librosa.feature.melspectrogram(data, n_mels = n_melspec)   \n",
    "            logspec = librosa.amplitude_to_db(melspec)\n",
    "            logspec = np.expand_dims(logspec, axis=-1)\n",
    "            X[cnt,] = logspec\n",
    "            \n",
    "        cnt += 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "'''\n",
    "3. Confusion matrix plot \n",
    "'''        \n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    '''Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    '''\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "# 4. Create the 2D CNN model \n",
    "'''\n",
    "def get_2d_conv_model(n):\n",
    "    ''' Create a standard deep 2D convolutional neural network'''\n",
    "    nclass = 6\n",
    "    inp = Input(shape=(n,862,1))  #2D matrix of 30 MFCC bands by 216 audio length.\n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    opt = optimizers.Adam(0.001)\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "# 5. Other functions \n",
    "'''\n",
    "class get_results:\n",
    "    '''\n",
    "    We're going to create a class (blueprint template) for generating the results based on the various model approaches. \n",
    "    So instead of repeating the functions each time, we assign the results into on object with its associated variables \n",
    "    depending on each combination:\n",
    "        1) MFCC with no augmentation  \n",
    "        2) MFCC with augmentation \n",
    "        3) Logmelspec with no augmentation \n",
    "        4) Logmelspec with augmentation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model_history, model ,X_test, y_test, labels):\n",
    "        self.model_history = model_history\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test             \n",
    "        self.labels = labels\n",
    "\n",
    "    def create_plot(self, model_history):\n",
    "        '''Check the logloss of both train and validation, make sure they are close and have plateau'''\n",
    "        plt.plot(model_history.history['loss'])\n",
    "        plt.plot(model_history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def create_results(self, model):\n",
    "        '''predict on test set and get accuracy results'''\n",
    "        opt = optimizers.Adam(0.001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    def confusion_results(self, X_test, y_test, labels, model):\n",
    "        '''plot confusion matrix results'''\n",
    "        preds = model.predict(X_test, \n",
    "                                 batch_size=16, \n",
    "                                 verbose=2)\n",
    "        preds=preds.argmax(axis=1)\n",
    "        preds = preds.astype(int).flatten()\n",
    "        preds = (lb.inverse_transform((preds)))\n",
    "\n",
    "        actual = y_test.argmax(axis=1)\n",
    "        actual = actual.astype(int).flatten()\n",
    "        actual = (lb.inverse_transform((actual)))\n",
    "\n",
    "        classes = labels\n",
    "        classes.sort()    \n",
    "\n",
    "        c = confusion_matrix(actual, preds)\n",
    "        print_confusion_matrix(c, class_names = classes)\n",
    "    \n",
    "    def accuracy_results_gender(self, X_test, y_test, labels, model):\n",
    "        '''Print out the accuracy score and confusion matrix heat map of the Gender classification results'''\n",
    "    \n",
    "        preds = model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=2)\n",
    "        preds=preds.argmax(axis=1)\n",
    "        preds = preds.astype(int).flatten()\n",
    "        preds = (lb.inverse_transform((preds)))\n",
    "\n",
    "        actual = y_test.argmax(axis=1)\n",
    "        actual = actual.astype(int).flatten()\n",
    "        actual = (lb.inverse_transform((actual)))\n",
    "        \n",
    "        # print(accuracy_score(actual, preds))\n",
    "        \n",
    "        actual = pd.DataFrame(actual).replace({'female_angry':'female'\n",
    "                   , 'female_disgust':'female'\n",
    "                   , 'female_fear':'female'\n",
    "                   , 'female_happy':'female'\n",
    "                   , 'female_sad':'female'\n",
    "                   , 'female_surprise':'female'\n",
    "                   , 'female_neutral':'female'\n",
    "                   , 'male_angry':'male'\n",
    "                   , 'male_fear':'male'\n",
    "                   , 'male_happy':'male'\n",
    "                   , 'male_sad':'male'\n",
    "                   , 'male_surprise':'male'\n",
    "                   , 'male_neutral':'male'\n",
    "                   , 'male_disgust':'male'\n",
    "                  })\n",
    "        preds = pd.DataFrame(preds).replace({'female_angry':'female'\n",
    "               , 'female_disgust':'female'\n",
    "               , 'female_fear':'female'\n",
    "               , 'female_happy':'female'\n",
    "               , 'female_sad':'female'\n",
    "               , 'female_surprise':'female'\n",
    "               , 'female_neutral':'female'\n",
    "               , 'male_angry':'male'\n",
    "               , 'male_fear':'male'\n",
    "               , 'male_happy':'male'\n",
    "               , 'male_sad':'male'\n",
    "               , 'male_surprise':'male'\n",
    "               , 'male_neutral':'male'\n",
    "               , 'male_disgust':'male'\n",
    "              })\n",
    "\n",
    "        classes = actual.loc[:,0].unique() \n",
    "        classes.sort()    \n",
    "\n",
    "        c = confusion_matrix(actual, preds)\n",
    "        print(accuracy_score(actual, preds))\n",
    "        print_confusion_matrix(c, class_names = classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170b67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('targets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c393c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = targets.groupby('gender_emotion')\n",
    "CREMA_sample = group.head(200)\n",
    "\n",
    "#CREMA_sample = targets[targets['gender']=='female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad99bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2400/2400 [03:28<00:00, 11.50it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_rate=44100\n",
    "audio_duration=10\n",
    "n_mfcc = 13\n",
    "mfcc = prepare_data(CREMA_sample, n = n_mfcc, aug = 0, mfcc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53e8911",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling2d_15\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_15/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,107,32].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 1, 107, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m X_test \u001b[38;5;241m=\u001b[39m (X_test \u001b[38;5;241m-\u001b[39m mean)\u001b[38;5;241m/\u001b[39mstd\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Build CNN model \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_2d_conv_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mfcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m model_history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), \n\u001b[1;32m     25\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mget_2d_conv_model\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m    135\u001b[0m x \u001b[38;5;241m=\u001b[39m BatchNormalization()(x)\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m Activation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[0;32m--> 137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mMaxPool2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)(x)\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/speech-emotion-recognition/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/speech-emotion-recognition/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2013\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   2010\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2012\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m-> 2013\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c_op\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"max_pooling2d_15\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_15/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,107,32].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 1, 107, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(mfcc\n",
    "                                                    , CREMA_sample.emotion\n",
    "                                                    , test_size=0.20\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = to_categorical(lb.fit_transform(y_train))\n",
    "y_test = to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "# Normalization as per the standard NN process\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Build CNN model \n",
    "model = get_2d_conv_model(n=n_mfcc)\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    batch_size=32, verbose = 2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e002ccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m get_results(\u001b[43mmodel_history\u001b[49m,model,X_test,y_test, CREMA_sample\u001b[38;5;241m.\u001b[39memotion\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      2\u001b[0m results\u001b[38;5;241m.\u001b[39mcreate_plot(model_history)\n\u001b[1;32m      3\u001b[0m results\u001b[38;5;241m.\u001b[39mcreate_results(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_history' is not defined"
     ]
    }
   ],
   "source": [
    "results = get_results(model_history,model,X_test,y_test, CREMA_sample.emotion.unique())\n",
    "results.create_plot(model_history)\n",
    "results.create_results(model)\n",
    "results.confusion_results(X_test, y_test, CREMA_sample.emotion.unique(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea2af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9e66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
