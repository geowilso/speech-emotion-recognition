{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e832cb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LayerNormalization' from 'keras.layers.normalization' (/Users/julian/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/normalization/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y_/q_7ncy856qz3xvy8zy_n1hzh0000gn/T/ipykernel_92739/3565323201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# Normalization layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSyncBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LayerNormalization' from 'keras.layers.normalization' (/Users/julian/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/normalization/__init__.py)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras import utils\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout,\n",
    "                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation, Input, Dense)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import scipy\n",
    "from scipy.stats import skew\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import warnings\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data Augmentation method   \n",
    "'''\n",
    "def speedNpitch(data):\n",
    "    \"\"\"\n",
    "    Speed and Pitch Tuning.\n",
    "    \"\"\"\n",
    "    # you can change low and high here\n",
    "    length_change = np.random.uniform(low=0.8, high = 1)\n",
    "    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n",
    "    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n",
    "    minlen = min(data.shape[0], tmp.shape[0])\n",
    "    data *= 0\n",
    "    data[0:minlen] = tmp[0:minlen]\n",
    "    return data\n",
    "\n",
    "'''\n",
    "2. Extracting the MFCC feature as an image (Matrix format).  \n",
    "'''\n",
    "def prepare_data(df, n, aug, mfcc):\n",
    "    X = np.empty(shape=(df.shape[0], n, 216, 1))\n",
    "    input_length = sampling_rate * audio_duration\n",
    "    \n",
    "    cnt = 0\n",
    "    for fname in tqdm(df.path):\n",
    "        file_path = fname\n",
    "        data, _ = librosa.load(file_path, sr=sampling_rate\n",
    "                               ,res_type=\"kaiser_fast\"\n",
    "                               ,duration=2.5\n",
    "                               ,offset=0.5\n",
    "                              )\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, int(input_length) - len(data) - offset), \"constant\")\n",
    "\n",
    "        # Augmentation? \n",
    "        if aug == 1:\n",
    "            data = speedNpitch(data)\n",
    "        \n",
    "        # which feature?\n",
    "        if mfcc == 1:\n",
    "            # MFCC extraction \n",
    "            MFCC = librosa.feature.mfcc(data, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "            MFCC = np.expand_dims(MFCC, axis=-1)\n",
    "            X[cnt,] = MFCC\n",
    "            \n",
    "        else:\n",
    "            # Log-melspectogram\n",
    "            melspec = librosa.feature.melspectrogram(data, n_mels = n_melspec)   \n",
    "            logspec = librosa.amplitude_to_db(melspec)\n",
    "            logspec = np.expand_dims(logspec, axis=-1)\n",
    "            X[cnt,] = logspec\n",
    "            \n",
    "        cnt += 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "'''\n",
    "3. Confusion matrix plot \n",
    "'''        \n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    '''Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    '''\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "# 4. Create the 2D CNN model \n",
    "'''\n",
    "def get_2d_conv_model(n):\n",
    "    ''' Create a standard deep 2D convolutional neural network'''\n",
    "    nclass = 12\n",
    "    inp = Input(shape=(n,216,1))  #2D matrix of 30 MFCC bands by 216 audio length.\n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    opt = optimizers.Adam(0.001)\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "# 5. Other functions \n",
    "'''\n",
    "class get_results:\n",
    "    '''\n",
    "    We're going to create a class (blueprint template) for generating the results based on the various model approaches. \n",
    "    So instead of repeating the functions each time, we assign the results into on object with its associated variables \n",
    "    depending on each combination:\n",
    "        1) MFCC with no augmentation  \n",
    "        2) MFCC with augmentation \n",
    "        3) Logmelspec with no augmentation \n",
    "        4) Logmelspec with augmentation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model_history, model ,X_test, y_test, labels):\n",
    "        self.model_history = model_history\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test             \n",
    "        self.labels = labels\n",
    "\n",
    "    def create_plot(self, model_history):\n",
    "        '''Check the logloss of both train and validation, make sure they are close and have plateau'''\n",
    "        plt.plot(model_history.history['loss'])\n",
    "        plt.plot(model_history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def create_results(self, model):\n",
    "        '''predict on test set and get accuracy results'''\n",
    "        opt = optimizers.Adam(0.001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    def confusion_results(self, X_test, y_test, labels, model):\n",
    "        '''plot confusion matrix results'''\n",
    "        preds = model.predict(X_test, \n",
    "                                 batch_size=16, \n",
    "                                 verbose=2)\n",
    "        preds=preds.argmax(axis=1)\n",
    "        preds = preds.astype(int).flatten()\n",
    "        preds = (lb.inverse_transform((preds)))\n",
    "\n",
    "        actual = y_test.argmax(axis=1)\n",
    "        actual = actual.astype(int).flatten()\n",
    "        actual = (lb.inverse_transform((actual)))\n",
    "\n",
    "        classes = labels\n",
    "        classes.sort()    \n",
    "\n",
    "        c = confusion_matrix(actual, preds)\n",
    "        print_confusion_matrix(c, class_names = classes)\n",
    "    \n",
    "    def accuracy_results_gender(self, X_test, y_test, labels, model):\n",
    "        '''Print out the accuracy score and confusion matrix heat map of the Gender classification results'''\n",
    "    \n",
    "        preds = model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=2)\n",
    "        preds=preds.argmax(axis=1)\n",
    "        preds = preds.astype(int).flatten()\n",
    "        preds = (lb.inverse_transform((preds)))\n",
    "\n",
    "        actual = y_test.argmax(axis=1)\n",
    "        actual = actual.astype(int).flatten()\n",
    "        actual = (lb.inverse_transform((actual)))\n",
    "        \n",
    "        # print(accuracy_score(actual, preds))\n",
    "        \n",
    "        actual = pd.DataFrame(actual).replace({'female_angry':'female'\n",
    "                   , 'female_disgust':'female'\n",
    "                   , 'female_fear':'female'\n",
    "                   , 'female_happy':'female'\n",
    "                   , 'female_sad':'female'\n",
    "                   , 'female_surprise':'female'\n",
    "                   , 'female_neutral':'female'\n",
    "                   , 'male_angry':'male'\n",
    "                   , 'male_fear':'male'\n",
    "                   , 'male_happy':'male'\n",
    "                   , 'male_sad':'male'\n",
    "                   , 'male_surprise':'male'\n",
    "                   , 'male_neutral':'male'\n",
    "                   , 'male_disgust':'male'\n",
    "                  })\n",
    "        preds = pd.DataFrame(preds).replace({'female_angry':'female'\n",
    "               , 'female_disgust':'female'\n",
    "               , 'female_fear':'female'\n",
    "               , 'female_happy':'female'\n",
    "               , 'female_sad':'female'\n",
    "               , 'female_surprise':'female'\n",
    "               , 'female_neutral':'female'\n",
    "               , 'male_angry':'male'\n",
    "               , 'male_fear':'male'\n",
    "               , 'male_happy':'male'\n",
    "               , 'male_sad':'male'\n",
    "               , 'male_surprise':'male'\n",
    "               , 'male_neutral':'male'\n",
    "               , 'male_disgust':'male'\n",
    "              })\n",
    "\n",
    "        classes = actual.loc[:,0].unique() \n",
    "        classes.sort()    \n",
    "\n",
    "        c = confusion_matrix(actual, preds)\n",
    "        print(accuracy_score(actual, preds))\n",
    "        print_confusion_matrix(c, class_names = classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3db9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA = '../raw_data/wav_files/'\n",
    "dir_list = os.listdir(CREMA)\n",
    "dir_list.sort()\n",
    "print(dir_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = []\n",
    "emotion = []\n",
    "path = []\n",
    "female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n",
    "          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n",
    "\n",
    "for i in dir_list: \n",
    "    part = i.split('_')\n",
    "    if int(part[0]) in female:\n",
    "        temp = 'female'\n",
    "    else:\n",
    "        temp = 'male'\n",
    "    gender.append(temp)\n",
    "    if part[2] == 'SAD' and temp == 'male':\n",
    "        emotion.append('male_sad')\n",
    "    elif part[2] == 'ANG' and temp == 'male':\n",
    "        emotion.append('male_angry')\n",
    "    elif part[2] == 'DIS' and temp == 'male':\n",
    "        emotion.append('male_disgust')\n",
    "    elif part[2] == 'FEA' and temp == 'male':\n",
    "        emotion.append('male_fear')\n",
    "    elif part[2] == 'HAP' and temp == 'male':\n",
    "        emotion.append('male_happy')\n",
    "    elif part[2] == 'NEU' and temp == 'male':\n",
    "        emotion.append('male_neutral')\n",
    "    elif part[2] == 'SAD' and temp == 'female':\n",
    "        emotion.append('female_sad')\n",
    "    elif part[2] == 'ANG' and temp == 'female':\n",
    "        emotion.append('female_angry')\n",
    "    elif part[2] == 'DIS' and temp == 'female':\n",
    "        emotion.append('female_disgust')\n",
    "    elif part[2] == 'FEA' and temp == 'female':\n",
    "        emotion.append('female_fear')\n",
    "    elif part[2] == 'HAP' and temp == 'female':\n",
    "        emotion.append('female_happy')\n",
    "    elif part[2] == 'NEU' and temp == 'female':\n",
    "        emotion.append('female_neutral')\n",
    "    else:\n",
    "        emotion.append('Unknown')\n",
    "    path.append(CREMA + i)\n",
    "    \n",
    "CREMA_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "CREMA_df['source'] = 'CREMA'\n",
    "CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "CREMA_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = CREMA_df.groupby('labels')\n",
    "CREMA_sample = group.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad99bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate=44100\n",
    "audio_duration=2.5\n",
    "n_mfcc = 30\n",
    "mfcc = prepare_data(CREMA_sample, n = n_mfcc, aug = 0, mfcc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(mfcc\n",
    "                                                    , CREMA_sample.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "# Normalization as per the standard NN process\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Build CNN model \n",
    "model = get_2d_conv_model(n=n_mfcc)\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    batch_size=32, verbose = 2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e002ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_results(model_history,model,X_test,y_test, CREMA_sample.labels.unique())\n",
    "results.create_plot(model_history)\n",
    "results.create_results(model)\n",
    "results.confusion_results(X_test, y_test, CREMA_sample.labels.unique(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d17c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "lewagon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
