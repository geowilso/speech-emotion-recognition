{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d893abf",
   "metadata": {},
   "source": [
    "# Data visualisation and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436aaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA = '../raw_data/wav_files/'\n",
    "dir_list = os.listdir(CREMA)\n",
    "dir_list.sort()\n",
    "print(dir_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0440f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gender = []\n",
    "emotion = []\n",
    "path = []\n",
    "female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n",
    "          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n",
    "\n",
    "for i in dir_list: \n",
    "    part = i.split('_')\n",
    "    if int(part[0]) in female:\n",
    "        temp = 'female'\n",
    "    else:\n",
    "        temp = 'male'\n",
    "    gender.append(temp)\n",
    "    \n",
    "    if part[2] == 'SAD':\n",
    "        emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        emotion.append('neutral')\n",
    "    else:\n",
    "        emotion.append('unknown')\n",
    "    path.append(CREMA + i)\n",
    "    \n",
    "CREMA_df = pd.DataFrame(emotion, columns = ['emotion'])\n",
    "#CREMA_df['source'] = 'CREMA'\n",
    "CREMA_df = pd.concat([CREMA_df,pd.DataFrame(gender, columns = ['gender'])],axis=1)\n",
    "CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo(df):\n",
    "    return f'{df[1]}_{df[0]}'\n",
    "\n",
    "def sad(x):\n",
    "    return 1 if x.lower() == 'sad' else 0\n",
    "\n",
    "def angry(x):\n",
    "    return 1 if x.lower() == 'angry' else 0\n",
    "\n",
    "def disgust(x):\n",
    "    return 1 if x.lower() == 'disgust' else 0\n",
    "\n",
    "def fear(x):\n",
    "    return 1 if x.lower() == 'fear' else 0\n",
    "\n",
    "def happy(x):\n",
    "    return 1 if x.lower() == 'happy' else 0\n",
    "\n",
    "def neutral(x):\n",
    "    return 1 if x.lower() == 'neutral' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07447db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_df['gender_emotion'] = CREMA_df.apply(combo,axis=1)\n",
    "CREMA_df['sad'] = CREMA_df['emotion'].apply(sad)\n",
    "CREMA_df['angry'] = CREMA_df['emotion'].apply(angry)\n",
    "CREMA_df['disgust'] = CREMA_df['emotion'].apply(disgust)\n",
    "CREMA_df['fear'] = CREMA_df['emotion'].apply(fear)\n",
    "CREMA_df['happy'] = CREMA_df['emotion'].apply(happy)\n",
    "CREMA_df['neutral'] = CREMA_df['emotion'].apply(neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f634ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREMA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(CREMA_df.emotion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7660a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,5))\n",
    "sns.histplot(CREMA_df.gender);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "sns.histplot(CREMA_df.gender_emotion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the well known Librosa library for this task \n",
    "fname = CREMA + '1091_IWW_SAD_XX.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "Audio(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100  \n",
    "y, sr = librosa.load(fname, sr=SAMPLE_RATE, duration = 5) # Chop audio at 5 secs... \n",
    "mfcc = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc = 5) # 5 MFCC components\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc)\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(fname, sr=SAMPLE_RATE, duration = 5) # Chop audio at 5 secs... \n",
    "melspec = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "\n",
    "# Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "log_S = librosa.amplitude_to_db(melspec)\n",
    "\n",
    "# Display the log mel spectrogram\n",
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+02.0f dB')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(fname, sr=SAMPLE_RATE, duration = 5) \n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "Audio(y_harmonic, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(y_percussive, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonic \n",
    "melspec = librosa.feature.melspectrogram(y_harmonic, sr=sr, n_mels=128)\n",
    "log_h = librosa.amplitude_to_db(melspec)\n",
    "\n",
    "# percussive\n",
    "melspec = librosa.feature.melspectrogram(y_percussive, sr=sr, n_mels=128)\n",
    "log_p = librosa.amplitude_to_db(melspec)\n",
    "\n",
    "# Display the log mel spectrogram of both harmonic and percussive\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.specshow(log_h, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.title('harmonic')\n",
    "plt.colorbar(format='%+02.0f dB')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.specshow(log_p, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.title('percussive')\n",
    "plt.colorbar(format='%+02.0f dB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ce715",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(fname, sr=SAMPLE_RATE, duration = 5)\n",
    "C = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "\n",
    "# Make a new figure\n",
    "plt.figure(figsize=(12,4))\n",
    "# To make sure that the colors span the full range of chroma values, set vmin and vmax\n",
    "librosa.display.specshow(C, sr=sr, x_axis='time', y_axis='chroma', vmin=0, vmax=1)\n",
    "plt.title('Chromagram')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8847bb3",
   "metadata": {},
   "source": [
    "# MFCC Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = librosa.load(fname, sr=SAMPLE_RATE, duration = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5208323",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c638bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = []\n",
    "mfcc = librosa.feature.mfcc(wav[0], sr=44000, n_mfcc=20)\n",
    "mfcc = mfcc.T\n",
    "mfccs.append(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c48f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77db707",
   "metadata": {},
   "source": [
    "# Padding All Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
